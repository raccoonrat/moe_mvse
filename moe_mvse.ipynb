{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "这是一个非常好的计划。在 Colab 上实现这个最小验证实验（MVES）是验证我们理论可行性的最快路径。\n",
        "\n",
        "为了保证代码的**可读性**、**可扩展性**和**可复现性**，我将采用以下策略来构建这个 Notebook：\n",
        "\n",
        "1.  **`transformers` 优先**：我们将尽可能使用 `transformers` 库的 `LogitsProcessor` API。这是一个专门用于在 `model.generate()` 过程中修改 logits 的标准接口，非常适合水印嵌入。\n",
        "2.  **效率与简洁的平衡**：LSH-GHW 方案需要在*生成时*获取*内部*的路由权重（RW）。我们将使用一种“预计算 (pre-pass)” 的方式在 `LogitsProcessor` 内部实现这一点。这虽然会轻微增加计算量（在 MVES 阶段可接受），但能使我们的代码**极度模块化**，并复用 `transformers` 强大的 `generate()` 函数。\n",
        "3.  **配置驱动**：所有实验参数（$L, m_e, \\delta$ 等）都将集中在一个配置对象中，方便快速迭代和扩展。\n",
        "\n",
        "**注意**：此代码是一个完整的、可运行的 Colab 示例。您需要一个**有 T4 GPU**的 Colab 运行时来加速 MoE 模型的推理。\n",
        "\n",
        "-----\n",
        "\n",
        "### Colab Notebook：MoE 水印最小验证实验 (MVES)"
      ],
      "metadata": {
        "id": "LB1GeoDhgOR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 0. 安装与环境设置\n",
        "# @markdown (运行此单元格以安装所需库)\n",
        "!pip install transformers datasets torch scipy sentencepiece accelerate bitsandbytes -q\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    LogitsProcessor,\n",
        "    LogitsProcessorList,\n",
        "    T5ForConditionalGeneration  # 我们将使用 Switch-T5 的基类\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# 忽略 transformers 的一些已知警告\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# 设置设备\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 为可复现性设置随机种子\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Gvuw2cgOR-",
        "outputId": "90ba15ab-f27d-43c5-9dc4-3df3f4f67cc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. 实验核心配置 (Config)\n",
        "# @markdown (所有实验参数都在这里定义，便于扩展和迭代)\n",
        "\n",
        "@dataclass\n",
        "class MVESConfig:\n",
        "    # --- 模型与数据配置 ---\n",
        "    MODEL_ID: str = \"google/switch-base-8\"  # MVES 关键：使用一个公开的 MoE 模型\n",
        "    DATASET_NAME: str = \"c4\"\n",
        "    DATASET_SPLIT: str = \"validation\"\n",
        "    NUM_SAMPLES: int = 50           # MVES 关键：快速迭代，使用小样本量\n",
        "    PROMPT_LENGTH: int = 50         # 续写任务的上下文长度\n",
        "    GEN_LENGTH: int = 100           # 生成文本的长度\n",
        "\n",
        "    # --- KGW 基线配置 ---\n",
        "    KGW_GAMMA: float = 0.5          # 绿表大小比例\n",
        "    KGW_DELTA: float = 2.0          # 绿表 logits 提升强度\n",
        "\n",
        "    # --- LSH-GHW (我们的方案) 配置 ---\n",
        "    LSH_L: int = 32                 # L: LSH 签名长度\n",
        "    LSH_ME: int = 1                 # m_e: 每个专家池的平均大小\n",
        "    LSH_MT: int = 200               # m_t: 每个词汇池的大小\n",
        "    LSH_DELTA_MAX: float = 2.0      # delta_max: 最大水印强度\n",
        "    # tau_low: 激活熵阈值，将在 \"校准\" 步骤中动态设置\n",
        "    LSH_TAU_LOW: float = 0.0\n",
        "\n",
        "    # --- 辅助参数 ---\n",
        "    BATCH_SIZE: int = 4\n",
        "    VOCAB_SIZE: int = 0             # 将在加载 tokenizer 后设置\n",
        "    NUM_EXPERTS: int = 0            # 将在加载 model 后设置\n",
        "    RW_DIM: int = 0                 # 将在加载 model 后设置\n",
        "\n",
        "# 实例化配置\n",
        "config = MVESConfig()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Cs7MeeAGgOR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    LogitsProcessor,\n",
        "    LogitsProcessorList,\n",
        "    T5ForConditionalGeneration  # 我们将使用 Switch-T5 的基类\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# 忽略 transformers 的一些已知警告\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# 设置设备\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 为可复现性设置随机种子\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# @title 1. 实验核心配置 (Config)\n",
        "# @markdown (所有实验参数都在这里定义，便于扩展和迭代)\n",
        "\n",
        "@dataclass\n",
        "class MVESConfig:\n",
        "    # --- 模型与数据配置 ---\n",
        "    MODEL_ID: str = \"google/switch-base-8\"  # MVES 关键：使用一个公开的 MoE 模型\n",
        "    DATASET_NAME: str = \"wikitext\" # Modified: Changed from c4 to wikitext\n",
        "    DATASET_CONFIG: str = \"wikitext-2-raw-v1\" # Modified: Changed config for wikitext\n",
        "    DATASET_SPLIT: str = \"validation\"\n",
        "    NUM_SAMPLES: int = 50           # MVES 关键：快速迭代，使用小样本量\n",
        "    PROMPT_LENGTH: int = 50         # 续写任务的上下文长度\n",
        "    GEN_LENGTH: int = 100           # 生成文本的长度\n",
        "\n",
        "    # --- KGW 基线配置 ---\n",
        "    KGW_GAMMA: float = 0.5          # 绿表大小比例\n",
        "    KGW_DELTA: float = 2.0          # 绿表 logits 提升强度\n",
        "\n",
        "    # --- LSH-GHW (我们的方案) 配置 ---\n",
        "    LSH_L: int = 32                 # L: LSH 签名长度\n",
        "    LSH_ME: int = 1                 # m_e: 每个专家池的平均大小\n",
        "    LSH_MT: int = 200               # m_t: 每个词汇池的大小\n",
        "    LSH_DELTA_MAX: float = 2.0      # delta_max: 最大水印强度\n",
        "    # tau_low: 激活熵阈值，将在 \"校准\" 步骤中动态设置\n",
        "    LSH_TAU_LOW: float = 0.0\n",
        "\n",
        "    # --- 辅助参数 ---\n",
        "    BATCH_SIZE: int = 4\n",
        "    VOCAB_SIZE: int = 0             # 将在加载 tokenizer 后设置\n",
        "    NUM_EXPERTS: int = 0            # 将在加载 model 后设置\n",
        "    RW_DIM: int = 0                 # 将在加载 model 后设置\n",
        "\n",
        "# 实例化配置\n",
        "config = MVESConfig()\n",
        "\n",
        "\n",
        "# @title 2. 加载模型、Tokenizer 与数据集\n",
        "\n",
        "def load_model_and_data(config):\n",
        "    \"\"\"加载 MoE 模型、Tokenizer 和数据集\"\"\"\n",
        "    print(f\"Loading tokenizer: {config.MODEL_ID}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID)\n",
        "\n",
        "    print(f\"Loading model: {config.MODEL_ID}\")\n",
        "    # 使用 8-bit 加载以节省 Colab 内存\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        config.MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # 更新配置中的模型特定参数\n",
        "    config.VOCAB_SIZE = model.config.vocab_size\n",
        "    # Switch-T5 (base-8) 在解码器中有 8 个专家\n",
        "    config.NUM_EXPERTS = model.config.num_experts # 修正: 使用 num_experts\n",
        "    # 路由器的输入维度。为了与LSH哈希的R_i维度匹配，这里将其设置为专家的数量。\n",
        "    config.RW_DIM = config.NUM_EXPERTS # FIX: Align RW_DIM with NUM_EXPERTS for LSH hashing\n",
        "\n",
        "    print(f\"Model loaded. Vocab size: {config.VOCAB_SIZE}, Num Experts: {config.NUM_EXPERTS}, RW Dim: {config.RW_DIM}\")\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    dataset = load_dataset(config.DATASET_NAME, config.DATASET_CONFIG, split=config.DATASET_SPLIT)\n",
        "    prompts = []\n",
        "    for item in dataset.take(config.NUM_SAMPLES):\n",
        "        # 对文本进行编码和解码，确保 prompt 长度一致\n",
        "        text = item['text'] # The key 'text' is common for wikitext datasets as well\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", max_length=config.PROMPT_LENGTH, truncation=True).input_ids\n",
        "        if tokens.shape[1] == config.PROMPT_LENGTH:\n",
        "            prompts.append(tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
        "        if len(prompts) >= config.NUM_SAMPLES:\n",
        "            break\n",
        "\n",
        "    print(f\"Loaded {len(prompts)} prompts.\")\n",
        "    return model, tokenizer, prompts\n",
        "\n",
        "model, tokenizer, prompts = load_model_and_data(config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading tokenizer: google/switch-base-8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: google/switch-base-8\n",
            "Model loaded. Vocab size: 32128, Num Experts: 8, RW Dim: 8\n",
            "Loading data...\n",
            "Loaded 20 prompts.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1C1mrMvgOR_",
        "outputId": "b118e082-c4ed-43d6-a670-44c40cbe67b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. 辅助函数 (Z-Score & 熵)\n",
        "\n",
        "def z_score(num_hits, num_scored, p0):\n",
        "    \"\"\"计算 Z-score (标准 KGW 检测统计量)\"\"\"\n",
        "    if num_scored == 0:\n",
        "        return 0.0\n",
        "\n",
        "    n = num_scored\n",
        "    x = num_hits\n",
        "\n",
        "    # 避免除以零\n",
        "    if n * p0 * (1 - p0) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # z = (x - n*p0) / sqrt(n*p0*(1-p0))\n",
        "    z = (x - n * p0) / np.sqrt(n * p0 * (1 - p0))\n",
        "    return z\n",
        "\n",
        "def entropy(logits):\n",
        "    \"\"\"计算 logits 分布的熵 H(r(x))\"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    return -torch.sum(probs * log_probs, dim=-1)\n",
        "\n",
        "def get_router_states(model, input_ids):\n",
        "    \"\"\"\n",
        "    (MVES 关键) \"预计算\" 步骤：获取内部路由状态。\n",
        "    这是一个轻量级的前向传播，只为了获取路由信息。\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            decoder_input_ids=input_ids, # 简化的 T5 推理\n",
        "            output_router_logits=True\n",
        "        )\n",
        "\n",
        "        # Access the router logits for the last MoE layer in the decoder.\n",
        "        # This typically returns a tuple of tuples: ((tensor(batch, seq_len, num_experts),),)\n",
        "        # We want the actual tensor, which is outputs.decoder_router_logits[0][0].\n",
        "        router_logits_tensor = outputs.decoder_router_logits[0][0].float()\n",
        "\n",
        "        # We need the router logits for the *last token* in the sequence.\n",
        "        # The input_ids are (batch_size, current_sequence_length).\n",
        "        # router_logits_tensor is (batch_size, current_sequence_length, num_experts).\n",
        "        # We extract the last token's router logits for all items in the batch.\n",
        "        # Since model.generate uses batch_size=1, we expect router_logits_tensor to be (1, current_sequence_length, num_experts).\n",
        "        # So, we want the logits for the last token, and then squeeze the batch dimension.\n",
        "        router_logits_for_current_step = router_logits_tensor[0, -1, :] # Shape: (num_experts,)\n",
        "\n",
        "        # 1. Router Weights (RW) vector R_i\n",
        "        R_i = router_logits_for_current_step.cpu()\n",
        "\n",
        "        # 2. Activation Entropy H(r(x))\n",
        "        H_r = entropy(router_logits_for_current_step).cpu()\n",
        "\n",
        "        # 3. Activated Expert Sigma(x) (top-k, k=1 for Switch-T5)\n",
        "        Sigma_x = torch.topk(router_logits_for_current_step, k=1, dim=-1).indices.cpu().squeeze()\n",
        "\n",
        "        return R_i, H_r, Sigma_x"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jPjBQknfgOR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. KGW 基线 (LogitsProcessor & Detector)\n",
        "\n",
        "class KGWLogitsProcessor(LogitsProcessor):\n",
        "    \"\"\"实现标准 KGW (红绿词表) 逻辑\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.gamma = config.KGW_GAMMA\n",
        "        self.delta = config.KGW_DELTA\n",
        "        self.vocab_size = config.VOCAB_SIZE\n",
        "        self.green_list_size = int(self.vocab_size * self.gamma)\n",
        "\n",
        "    def _get_green_list(self, last_token_id):\n",
        "        \"\"\"使用 last_token_id 作为种子生成确定性的绿表\"\"\"\n",
        "        # (为了简单，我们使用 torch 的伪随机，而非 KGW 的哈希)\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(last_token_id)\n",
        "        indices = torch.randperm(self.vocab_size, generator=g)\n",
        "        green_list = indices[:self.green_list_size]\n",
        "        return green_list\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        # 1. 获取上一个 token\n",
        "        last_token_id = input_ids[0, -1].item()\n",
        "\n",
        "        # 2. 获取绿表\n",
        "        green_list = self._get_green_list(last_token_id)\n",
        "\n",
        "        # 3. 施加扰动 (boost)\n",
        "        scores[0, green_list] = scores[0, green_list] + self.delta\n",
        "        return scores\n",
        "\n",
        "def detect_kgw(text, tokenizer, config):\n",
        "    \"\"\"KGW 检测器\"\"\"\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids[0].to(DEVICE)\n",
        "    processor = KGWLogitsProcessor(config)\n",
        "\n",
        "    num_hits = 0\n",
        "    num_scored = len(tokens) - 1\n",
        "\n",
        "    for i in range(1, len(tokens)):\n",
        "        last_token_id = tokens[i-1].item()\n",
        "        current_token_id = tokens[i].item()\n",
        "\n",
        "        green_list = processor._get_green_list(last_token_id)\n",
        "\n",
        "        if current_token_id in green_list:\n",
        "            num_hits += 1\n",
        "\n",
        "    # H0 期望\n",
        "    p0 = config.KGW_GAMMA\n",
        "    return z_score(num_hits, num_scored, p0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "QeNHt9v4gOSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. LSH-GHW (我们的方案) - 辅助类 (Keys)\n",
        "# @markdown (这个类管理 LSH-GHW 的所有秘密密钥)\n",
        "\n",
        "class LSH_GHW_Keys:\n",
        "    \"\"\"管理 LSH-GHW 方案的所有密钥 (配置文件)\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.L = config.LSH_L\n",
        "        self.Me = config.LSH_ME\n",
        "        self.Mt = config.LSH_MT\n",
        "        self.NumExperts = config.NUM_EXPERTS\n",
        "        self.VocabSize = config.VOCAB_SIZE\n",
        "        self.RwDim = config.RW_DIM\n",
        "\n",
        "        # 1. K_lsh: L x d_RW 随机投影向量\n",
        "        self.K_lsh = torch.randn((self.L, self.RwDim))\n",
        "\n",
        "        # 2. E_Pools: L 个专家池\n",
        "        self.E_Pools = []\n",
        "        for _ in range(self.L):\n",
        "            indices = np.random.choice(self.NumExperts, self.Me, replace=False)\n",
        "            self.E_Pools.append(set(indices))\n",
        "\n",
        "        # 3. T_Pools: n 个词汇池 (每个专家一个)\n",
        "        self.T_Pools = []\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(42) # 确保词汇池可复现\n",
        "        for _ in range(self.NumExperts):\n",
        "            indices = torch.randperm(self.VocabSize, generator=g)[:self.Mt]\n",
        "            self.T_Pools.append(set(indices.tolist()))\n",
        "\n",
        "        # H0 期望的 p0\n",
        "        # E[|E_green|] = L * 0.5 * Me\n",
        "        # E[|Sigma_x|] = k (k=1 for Switch-T5)\n",
        "        # E[|Sigma_wm|] approx E[|E_green|] * (k / NumExperts)\n",
        "        # E[|G_i|] approx E[|Sigma_wm|] * Mt\n",
        "        expected_sigma_wm = (self.L * 0.5 * self.Me) * (1 / self.NumExperts)\n",
        "        expected_gi_size = expected_sigma_wm * self.Mt\n",
        "        self.p0 = expected_gi_size / self.VocabSize\n",
        "        if self.p0 > 1.0: self.p0 = 1.0\n",
        "        print(f\"LSH-GHW H0 (p0) = {self.p0:.6f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Opc-GlMcgOSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. LSH-GHW (我们的方案) - LogitsProcessor\n",
        "# @markdown (MVES 核心：实现 LSH-GHW 嵌入逻辑)\n",
        "\n",
        "class LSH_GHW_LogitsProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    (MVES 核心) 实现 LSH-GHW (分层) 逻辑。\n",
        "    使用 \"pre-pass\" 方式获取内部状态。\n",
        "    \"\"\"\n",
        "    def __init__(self, model, keys, config):\n",
        "        self.model = model\n",
        "        self.keys = keys\n",
        "        self.config = config\n",
        "\n",
        "    def _get_lsh_ghw_state(self, input_ids):\n",
        "        \"\"\"\n",
        "        执行方案的 步骤 1-4。\n",
        "        这是 MVES 核心逻辑的 \"预计算\" 部分。\n",
        "        \"\"\"\n",
        "        # 1. 获取内部状态\n",
        "        R_i, H_r, Sigma_x = get_router_states(self.model, input_ids.to(DEVICE))\n",
        "\n",
        "        # DEBUG print\n",
        "        print(f\"DEBUG from LogitsProcessor: R_i shape: {R_i.shape}, K_lsh.T shape: {self.keys.K_lsh.T.shape}\")\n",
        "\n",
        "        # 2. 语义门控 (LSH)\n",
        "        # R_i (d_RW) @ K_lsh.T (d_RW, L) -> S_i_cont (L)\n",
        "        S_i_cont = R_i.float() @ self.keys.K_lsh.T.float()\n",
        "        S_i_binary = (S_i_cont > 0) # L-bit 签名\n",
        "\n",
        "        E_green = set()\n",
        "        for j in range(self.keys.L):\n",
        "            if S_i_binary[j]:\n",
        "                E_green.update(self.keys.E_Pools[j])\n",
        "\n",
        "        # 3. 路由门控 (交集)\n",
        "        Sigma_watermark = E_green.intersection({Sigma_x.item()})\n",
        "\n",
        "        return H_r.item(), Sigma_watermark\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        # 1. 获取 LSH-GHW 状态\n",
        "        H_r, Sigma_watermark = self._get_lsh_ghw_state(input_ids)\n",
        "\n",
        "        # 2. 计算动态强度 (权衡点)\n",
        "        delta_adaptive = 0.0\n",
        "        if H_r >= self.config.LSH_TAU_LOW:\n",
        "            # 只有在模型 \"不确定\" (高熵) 时才加水印\n",
        "            delta_adaptive = self.config.LSH_DELTA_MAX\n",
        "\n",
        "        # 3. 构建绿色词表 G_i\n",
        "        G_i = set()\n",
        "        for k in Sigma_watermark:\n",
        "            G_i.update(self.keys.T_Pools[k])\n",
        "\n",
        "        # 4. 施加扰动 (boost)\n",
        "        if delta_adaptive > 0 and G_i:\n",
        "            green_list_indices = torch.tensor(list(G_i), dtype=torch.long, device=scores.device)\n",
        "            scores[0, green_list_indices] += delta_adaptive\n",
        "\n",
        "        return scores"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nFH-8hmGgOSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. LSH-GHW (我们的方案) - Detector\n",
        "\n",
        "def detect_lsh_ghw(text, tokenizer, model, keys, config):\n",
        "    \"\"\"LSH-GHW 检测器\"\"\"\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
        "\n",
        "    num_hits = 0\n",
        "    num_scored = 0\n",
        "\n",
        "    # 我们从第2个 token 开始检测 (需要至少1个 token 作为上下文)\n",
        "    for i in range(1, len(tokens)):\n",
        "        C_i = tokens[:i].unsqueeze(0).to(DEVICE) # 上下文\n",
        "        t_i = tokens[i].item()                   # 实际 token\n",
        "\n",
        "        # 1. 重构状态 (使用干净模型)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=C_i,\n",
        "                decoder_input_ids=C_i, # 简化的 T5 推理\n",
        "                output_router_logits=True\n",
        "            )\n",
        "\n",
        "        router_logits = outputs.decoder_router_logits[-1][:, -1, :]\n",
        "        R_i = router_logits.float().mean(dim=0).cpu()\n",
        "        H_r = entropy(router_logits).cpu().item()\n",
        "        Sigma_x = torch.topk(router_logits, k=1, dim=-1).indices.cpu().squeeze().item()\n",
        "\n",
        "        # 2. 重构 LSH 签名\n",
        "        S_i_cont = R_i.float() @ keys.K_lsh.T.float()\n",
        "        S_i_binary = (S_i_cont > 0)\n",
        "\n",
        "        E_green = set()\n",
        "        for j in range(keys.L):\n",
        "            if S_i_binary[j]:\n",
        "                E_green.update(keys.E_Pools[j])\n",
        "\n",
        "        # 3. 重构 Sigma_watermark 和 G_i\n",
        "        Sigma_watermark = E_green.intersection({Sigma_x})\n",
        "        G_i = set()\n",
        "        for k in Sigma_watermark:\n",
        "            G_i.update(keys.T_Pools[k])\n",
        "\n",
        "        # 4. 重构强度决策 (我们是否 *本应* 在此加水印?)\n",
        "        if H_r >= config.LSH_TAU_LOW:\n",
        "            num_scored += 1\n",
        "            if t_i in G_i:\n",
        "                num_hits += 1\n",
        "\n",
        "    return z_score(num_hits, num_scored, keys.p0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "eabx4VJQgOSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. 步骤 8: 校准 (Calibration)\n",
        "# @markdown (MVES 关键：动态设置 tau_low 阈值)\n",
        "\n",
        "def calibrate_tau(model, tokenizer, prompts, config, percentile=25):\n",
        "    \"\"\"\n",
        "    运行模型，收集激活熵 H(r(x)) 的分布。\n",
        "    设置 tau_low 为分布的 `percentile` 分位数。\n",
        "    (e.g., percentile=25 意味着我们在 75% \"最不确定\" 的 token 上加水印)\n",
        "    \"\"\"\n",
        "    print(\"Starting calibration for LSH_TAU_LOW...\")\n",
        "    entropies = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "        # 只取一个 token 来获取其熵\n",
        "        _, H_r, _ = get_router_states(model, input_ids)\n",
        "        entropies.append(H_r.item())\n",
        "\n",
        "    tau_low = np.percentile(entropies, percentile)\n",
        "    config.LSH_TAU_LOW = tau_low\n",
        "    print(f\"Calibration complete. LSH_TAU_LOW set to: {tau_low:.4f}\")\n",
        "\n",
        "# 运行校准\n",
        "calibrate_tau(model, tokenizer, prompts, config, percentile=25)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting calibration for LSH_TAU_LOW...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-899466303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 运行校准\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcalibrate_tau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-899466303.py\u001b[0m in \u001b[0;36mcalibrate_tau\u001b[0;34m(model, tokenizer, prompts, config, percentile)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 只取一个 token 来获取其熵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_router_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mentropies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2858821559.py\u001b[0m in \u001b[0;36mget_router_states\u001b[0;34m(model, input_ids)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Since model.generate uses batch_size=1, we expect router_logits_tensor to be (1, current_sequence_length, num_experts).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# So, we want the logits for the last token, and then squeeze the batch dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mrouter_logits_for_current_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouter_logits_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Shape: (num_experts,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# 1. Router Weights (RW) vector R_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "Cm7cB0l0gOSB",
        "outputId": "7fd8e78f-5220-43cd-9c59-9140a1a0a485"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. 实验执行：文本生成 (Generation)\n",
        "\n",
        "# 存储所有生成结果\n",
        "results = {\n",
        "    \"prompts\": prompts,\n",
        "    \"C1_Clean\": [],\n",
        "    \"C2_KGW\": [],\n",
        "    \"C3_LSH_GHW\": [],\n",
        "}\n",
        "\n",
        "# 初始化 LSH-GHW 密钥\n",
        "lsh_keys = LSH_GHW_Keys(config)\n",
        "\n",
        "# 初始化 LogitsProcessors\n",
        "kgw_processor = KGWLogitsProcessor(config)\n",
        "lsh_ghw_processor = LSH_GHW_LogitsProcessor(model, lsh_keys, config)\n",
        "\n",
        "print(\"Starting text generation...\")\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating sample {i+1}/{len(prompts)}...\")\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "\n",
        "    # --- C1: 干净模型 (Baseline) ---\n",
        "    outputs_c1 = model.generate(\n",
        "        input_ids,\n",
        "        max_length=config.PROMPT_LENGTH + config.GEN_LENGTH,\n",
        "        do_sample=True, top_k=10\n",
        "    )\n",
        "    results[\"C1_Clean\"].append(tokenizer.decode(outputs_c1[0], skip_special_tokens=True))\n",
        "\n",
        "    # --- C2: KGW 基线 ---\n",
        "    outputs_c2 = model.generate(\n",
        "        input_ids,\n",
        "        logits_processor=LogitsProcessorList([kgw_processor]),\n",
        "        max_length=config.PROMPT_LENGTH + config.GEN_LENGTH,\n",
        "        do_sample=True, top_k=10\n",
        "    )\n",
        "    results[\"C2_KGW\"].append(tokenizer.decode(outputs_c2[0], skip_special_tokens=True))\n",
        "\n",
        "    # --- C3: LSH-GHW (我们的方案) ---\n",
        "    outputs_c3 = model.generate(\n",
        "        input_ids,\n",
        "        logits_processor=LogitsProcessorList([lsh_ghw_processor]),\n",
        "        max_length=config.PROMPT_LENGTH + config.GEN_LENGTH,\n",
        "        do_sample=True, top_k=10\n",
        "    )\n",
        "    results[\"C3_LSH_GHW\"].append(tokenizer.decode(outputs_c3[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"Generation complete.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "q3EjitvrgOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. 实验执行：指标评估 (PPL & Z-Score)\n",
        "# @markdown (计算文本质量 PPL 和可检测性 Z-Score)\n",
        "\n",
        "# (注意：PPL 计算很慢。在 MVES 中，我们可以只看 Z-Score，或抽样 PPL)\n",
        "# (为简洁起见，我们跳过 PPL，重点关注 Z-Score，这已足够验证)\n",
        "\n",
        "print(\"Starting evaluation...\")\n",
        "report_data = []\n",
        "\n",
        "for i in range(len(prompts)):\n",
        "    # 获取 C1, C2, C3 的文本\n",
        "    text_c1 = results[\"C1_Clean\"][i]\n",
        "    text_c2 = results[\"C2_KGW\"][i]\n",
        "    text_c3 = results[\"C3_LSH_GHW\"][i]\n",
        "\n",
        "    # --- 评估可检测性 (Z-Score) ---\n",
        "\n",
        "    # 1. 检测 KGW (TP 和 FP)\n",
        "    z_c2_as_c2 = detect_kgw(text_c2, tokenizer, config) # 真阳性 TP\n",
        "    z_c1_as_c2 = detect_kgw(text_c1, tokenizer, config) # 假阳性 FP\n",
        "\n",
        "    # 2. 检测 LSH-GHW (TP 和 FP)\n",
        "    z_c3_as_c3 = detect_lsh_ghw(text_c3, tokenizer, model, lsh_keys, config) # TP\n",
        "    z_c1_as_c3 = detect_lsh_ghw(text_c1, tokenizer, model, lsh_keys, config) # FP\n",
        "\n",
        "    report_data.append({\n",
        "        \"sample_id\": i,\n",
        "        \"z_KGW_TP\": z_c2_as_c2,\n",
        "        \"z_KGW_FP\": z_c1_as_c2,\n",
        "        \"z_LSH_TP\": z_c3_as_c3,\n",
        "        \"z_LSH_FP\": z_c1_as_c3,\n",
        "    })\n",
        "\n",
        "report_df = pd.DataFrame(report_data)\n",
        "print(\"Evaluation complete.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EmqBNC8TgOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 11. 实验执行：语义鲁棒性 (R_input) 评估\n",
        "# @markdown (这是我们方案的核心优势验证)\n",
        "\n",
        "print(\"Starting R_input (Robustness) evaluation...\")\n",
        "\n",
        "# MVES 关键：使用一个简单的、模拟的释义攻击 (Paraphrase Attack)\n",
        "# 真实的实验需要一个 T5 释义模型\n",
        "def run_paraphrase_attack(prompt):\n",
        "    # 简单攻击：添加标点，改变大小写\n",
        "    return prompt.strip() + \". \" + \"In other words,\"\n",
        "\n",
        "robustness_data = []\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Running robustness attack {i+1}/{len(prompts)}...\")\n",
        "\n",
        "    # 1. 制造释义 prompt\n",
        "    para_prompt = run_paraphrase_attack(prompt)\n",
        "    para_input_ids = tokenizer(para_prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "\n",
        "    # 2. 用 KGW 生成释义文本\n",
        "    outputs_c2_para = model.generate(\n",
        "        para_input_ids,\n",
        "        logits_processor=LogitsProcessorList([kgw_processor]),\n",
        "        max_length=config.PROMPT_LENGTH + config.GEN_LENGTH,\n",
        "        do_sample=True, top_k=10\n",
        "    )\n",
        "    text_c2_para = tokenizer.decode(outputs_c2_para[0], skip_special_tokens=True)\n",
        "\n",
        "    # 3. 用 LSH-GHW 生成释义文本\n",
        "    outputs_c3_para = model.generate(\n",
        "        para_input_ids,\n",
        "        logits_processor=LogitsProcessorList([lsh_ghw_processor]),\n",
        "        max_length=config.PROMPT_LENGTH + config.GEN_LENGTH,\n",
        "        do_sample=True, top_k=10\n",
        "    )\n",
        "    text_c3_para = tokenizer.decode(outputs_c3_para[0], skip_special_tokens=True)\n",
        "\n",
        "    # 4. 检测 Z-Score (看衰减)\n",
        "    z_c2_para_decay = detect_kgw(text_c2_para, tokenizer, config)\n",
        "    z_c3_para_decay = detect_lsh_ghw(text_c3_para, tokenizer, model, lsh_keys, config)\n",
        "\n",
        "    robustness_data.append({\n",
        "        \"sample_id\": i,\n",
        "        \"z_KGW_Robust\": z_c2_para_decay,\n",
        "        \"z_LSH_Robust\": z_c3_para_decay,\n",
        "    })\n",
        "\n",
        "robust_df = pd.DataFrame(robustness_data)\n",
        "print(\"Robustness evaluation complete.\")\n",
        "\n",
        "# 合并报告\n",
        "report_df = report_df.merge(robust_df, on=\"sample_id\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "e71uSBo3gOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 12. 最终结果报告\n",
        "# @markdown (显示所有指标的平均值)\n",
        "\n",
        "# --- 计算平均值 ---\n",
        "avg_z_kgw_tp = report_df['z_KGW_TP'].mean()\n",
        "avg_z_kgw_fp = report_df['z_KGW_FP'].mean()\n",
        "avg_z_kgw_robust = report_df['z_KGW_Robust'].mean()\n",
        "\n",
        "avg_z_lsh_tp = report_df['z_LSH_TP'].mean()\n",
        "avg_z_lsh_fp = report_df['z_LSH_FP'].mean()\n",
        "avg_z_lsh_robust = report_df['z_LSH_Robust'].mean()\n",
        "\n",
        "# --- 计算衰减 (Z-score Decay) ---\n",
        "# KGW 不关心语义，其衰减应接近 0 (Z-Score 不变)\n",
        "# LSH-GHW 关心语义，我们期望衰减 > 0，但 Z-Score 仍远高于 FP\n",
        "decay_kgw = avg_z_kgw_tp - avg_z_kgw_robust\n",
        "decay_lsh = avg_z_lsh_tp - avg_z_lsh_robust\n",
        "\n",
        "# --- 打印报告 ---\n",
        "summary = pd.DataFrame({\n",
        "    \"Metric\": [\n",
        "        \"Avg. Z-Score (TP)\",\n",
        "        \"Avg. Z-Score (FP)\",\n",
        "        \"Avg. Z-Score (Robustness Attack)\",\n",
        "        \"Z-Score Decay (TP - Robust)\"\n",
        "    ],\n",
        "    \"C2 (KGW Baseline)\": [\n",
        "        f\"{avg_z_kgw_tp:.2f}\",\n",
        "        f\"{avg_z_kgw_fp:.2f}\",\n",
        "        f\"{avg_z_kgw_robust:.2f}\",\n",
        "        f\"{decay_kgw:.2f}\"\n",
        "    ],\n",
        "    \"C3 (LSH-GHW)\": [\n",
        "        f\"{avg_z_lsh_tp:.2f}\",\n",
        "        f\"{avg_z_lsh_fp:.2f}\",\n",
        "        f\"{avg_z_lsh_robust:.2f}\",\n",
        "        f\"{decay_lsh:.2f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"--- MVES 最终报告 ---\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "print(\"\\n--- 结果分析 (期望) ---\")\n",
        "print(f\"1. 可检测性: LSH-GHW (TP={avg_z_lsh_tp:.2f}) 和 KGW (TP={avg_z_kgw_tp:.2f}) 都应远高于 FP (Z > 4.0)。\")\n",
        "print(f\"2. 隐蔽性: LSH-GHW (FP={avg_z_lsh_fp:.2f}) 和 KGW (FP={avg_z_kgw_fp:.2f}) 都应接近 0。\")\n",
        "print(f\"3. 语义鲁棒性 (核心):\")\n",
        "print(f\"   - KGW 不关心语义，其 Z-Score 几乎不衰减 (Decay={decay_kgw:.2f})。\")\n",
        "print(f\"   - LSH-GHW 依赖语义，我们*期望* Z-Score 衰减 (Decay={decay_lsh:.2f})。\")\n",
        "print(f\"   - **成功标准**: 尽管衰减了，LSH-GHW 的鲁棒性 Z-Score ({avg_z_lsh_robust:.2f}) 仍然远高于 FP 阈值。\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "JL_ijiq9gOSB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}